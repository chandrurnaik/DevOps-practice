Jenkins cicd process (just a breif)
=> we are using source code management tool github in our company 
=> and integrated github to jenkins with git webhook
=> whenever developer push changes to remote repo, by webhook jenkins job automatically triggers
=> in jenkis using declarative pipeline job we are inplementing contineous integration and delivery approach 
=> and also we are maintaining jenkinsfiles in github only, just calling it in jenkins pipeline job
=> jenkinsfile involves ci stages like checkout, build, code scan, image build, image scan, 
   image push
=> we are maintaing dockerfile in github and using it in image build stage 
=> jenkinsfile involves cd stages like Deploy or update deployment file 
=> both build, push docker image we are doing it by shell scripting 
=> deploying to kubernetes by helm, again by helm commands 
=> we are maintaining all builds, docker images and helm chart in jfrog artifactory 

Default port of jfrog is => 9000 
How to integrate jfrog to jenkins 
   => Install plugin Artifactory in jenkins => go to Manage Jenkins => system => search for jfrog
     => set jfrog url and username, password 
   
   => using in pipeline => 2 ways => using jfrog cli and using blocks like rtupload and all (usually i refer documentory)
   => for jfrog cli we need to install Jfrog plugin 
   => we install and run jfrog artifactory in ec2 instance just like jenkins

How to integrate sonarqube to jenkins 
  => for this install sonarqube plugin in jenkins 
  => while code scan stage in pipeline, we need to pass sonarqube url 
  => and also just like jenkins server, have install and run sonarqube on ec2 instance and moniter 
  => Prerequisites:
     => SonarQube Server: Make sure you have a SonarQube server running and accessible to your Jenkins instance.
     => SonarQube Scanner: Install the SonarQube Scanner on the Jenkins machine or configure it as part of your 
        Jenkins environment.
     => Configure SonarQube in Jenkins: Configure SonarQube within Jenkins. This usually involves providing the 
        SonarQube server URL and authentication details.
        => steps: To configure the plugin, go to the Jenkins plugin manager and click the "Manage Plugins" button. 
        Then, click the "SonarQube Scanner" plugin and click the "Configure" button. In the configuration window, 
        enter the following information: SonarQube server URL and authentication details

  => pipeline code: 
     pipeline {
    agent any

    stages {
        stage('SonarQube Scan') {
            steps {
                // Checkout code from your repository
                checkout scm

                // Run SonarQube analysis
                script {
                    def scannerHome = tool 'sonar'; // Assuming 'sonar' is configured in Jenkins tools
                    withSonarQubeEnv('SonarQube_Server') {
                        sh "${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=your_project_key 
                        -Dsonar.sources=src"
                    }
                }
            }
        }

        // Other stages in your pipeline...
    }
    
    // Post-build actions or notifications...
}

How to do image scan in cicd jenkins pipeline using rapid 7:
=> To perform image scanning in your CI/CD Jenkins pipeline using Rapid7, follow these steps:
   => Install the Rapid7 InsightVM Container Image Scanner plugin and configured within your CI/CD environment.
   => Set up Credentials: Store any required credentials or API keys securely within Jenkins to 
      access Rapid7's services.
=> Hereâ€™s an example of what a Jenkins pipeline step might look like for initiating a Rapid7 scan:
   pipeline {
    agent any
    stages {
        stage('Rapid7 Image Scan') {
            steps {
                script {
                    // Fetch required credentials
                    withCredentials([usernamePassword(credentialsId: 'rapid7Credentials', 
                    usernameVariable: 'USERNAME', passwordVariable: 'PASSWORD')]) {
                        
                        // Define your Rapid7 scan command/API call
                        def imageToScan = "your_image_name:tag"
                        def apiKey = "YOUR_RAPID7_API_KEY"
                        
                        // Example command or API call to trigger the scan
                        sh "rapid7-tool scan-image -i $imageToScan --api-key $apiKey"
                        
                        // Or use HTTP requests or appropriate Rapid7 SDK methods to trigger the scan
                        // Example: httpRequest "https://api.rapid7.com/v1/scans"
                    }
                }
            }
        }
        // Additional stages for deployment, testing, etc.
    }
    // Post-build actions, notifications, etc.
}


1. Default ports of monitoring tools 
   Prometheus => 9090 
       => can change port in prometheus.yaml file 
   Grafana => 3000 
       => can change port in grafana.ini file
   ElasticSearch => 9200 
       => can change port in ElasticSearch.yaml file
   Kibana => 5601 
       => can change port in Kibana.yaml file

2. what is inode in linux 
   An inode in Linux is a data structure that stores information about a file or directory. Each file and 
   directory on a Linux filesystem has its own inode. 
   The inode contains information such as the file's size, permissions, owner, and location on the disk.

3. adduser command in linux 
   it will do both useradd and passwd in a single execution 

4. what is jombie process in linux 
   A zombie process in Linux is a process that has completed its execution, but its parent process has 
   not collected its exit status. This means that the process is still visible in the process table, 
   but it is no longer using any system resources.

5. how to create images out of docker container 
   using command => docker commit <container_name> <new_image_name>:<tag>

6. braching starturgies that you are following in your company 
   Basically we had 4 branches within the vcs 
   1. features branch => deployed to dev environment
   2. main branch => deployed to stagging or QA environment
   3. realease branch => deployed to pre-prod environment and later promoted to prod environment
   4. hotfix branch => any change request by customer is maintained in hotfix branch and later
                       merge to all the branches
    
   we had seperate jenkins cicd pipelines for each branches 


   features branch: any new features will be devoped by engineering team in features branch.
   and when an developer push changes to features branch a jenkins pipeline will get triggered
   will be deployed to dev environment and if everything looks fine it will be promoted to main branch
   by developer

   main branch: it is where actuall application is going on. any minor changes with application will be 
   done here, as a developer push changes to main branch a jenkins pipeline will get triggered 
   and get deployed to QA env, where app is tested for 2-4 days and everything looks fine there.
   later main branch get merge with release branch by devops team.

   realease branch: as main barnch get merged with realease branch by devops team again another jenkins 
   pipeline get triggred and deployed first to pre-prod (where it run some more set test) and later get 
   promoted and one more jenkins pipeline get triggered by us and deployed to prod env.


7. Disaster Recovery management
   => It is a set of procedure and measures to recover and restore system to its normal operations in the 
      event of major outage, natural disaster or failures.
   => basically it is a another set of prod evn in different region 
   => we have RTO and RPO defined for DR in SLA 
   => both RTO and RPO totally depends on individual projects 
   => in our company 
       RTO is 30min
       RPO is latest 4hrs of data 
   => DR is like a replica of prod evn and if major outage happens with prod env as per RTO 
      we have restore env within 30min and restore data backup of last 4 hrs from the time 
      of major outage 
   => only critical resources are present in DR env and all are of stop state, so to cost optimize it.
   => to implement DR i have done some automation by python script 
      1. created a backup schedule for every 4hrs (starting from 12AM)
      2. copy the backup snapshot from source region to DR region 
      3. Restoring backup 
      4. cleaning up the old backups every week  

8. 3-Tier Architecture (explain only when its highly required)
   => 3-tier Architecture app module 
      => frontend (ui)
      => backend 
      => database 
   => In my previous projects i have implemented 3-tier Architecture in instances 
      Now we have migrated to kubernetes 
   => only explain this if they asked to explain old 3-teir architecture on instances 
      => first we created VPC with specified CIDR range to it 
      => create 3 private subnet with 2 AZ each 
         => private-subnet-frontend 
         => private-subnet-backend 
         => private-subnet-database
      => private-subnet-frontend
         => using Auto Scaling Group we deploy frontend in 2 instances resides in 2 Az's 
      => private-subnet-backend
         => using Auto Scaling Group we deploy backend in 2 instances resides in 2 Az's
      => private-subnet-database
         => here i created a AWS RDS with one primary db in 1 AZ and one more secondary db
            another AZ (so to privide high availability)
      => created ELB on public subnet of vpc in both AZ's 
      => firstly user request come to Route53 than its routed to ELB to frontend 
      => again by frontend ec2 instance resquest is routed to ALB of backend 
      => then from backend to database
      => then from backend response to send to backend and frontend
      => this was a 3-tier architecture module i have implemented in my pervious project 

9. prometheus Architecture
   => prometheus is a open source and widely used monitoring and alerting toolkit
   => its architecture has multiple componenets 
      1. Prometheus Server
      2. Exporter 
      3. Pushgateway
      4. Alertmanager
      3. Grafana Visualization and dashboard tool 
   => Prometheus Server:
      It is a first core component of prometheus, which will scrap the metrics data from 
      target endpoints on specified scrape interval using Data Replication Worker and store 
      the data in its own Time-seriesed Dadatabase store it in node HDD or SSD or containers
      and process the data by HTTP server using promQL language and made data avaialable to Grafana 
      by http servers.
      1. Data Retrival Worker
      2. storing 
      3. HTTP Server 
   => Exporters:
      The exporters are a scripts or simplified application that run along with application which
      will collect and expose the data in a prometheus data formates
   => prometheus works on pull based data model, target machines dont need to push data 
   => Pushgateway:
      If we have any short-lived base job that is not feasible to pull data to prometheus servers 
      prometheus provided Pushgateway componenets it is a intermidiary for collecting push data 
      and it is pulled by prometheus server 
   => Alertmanager:
      It is componenet using which we can get alerts via emails or slacks when the metrics met the 
      alerting rules set in prometheus servers
   => Grafana:
      It is Visualization and dashboarding tools best suited to prometheus
      => we configure prometheus as Data source in garfana and as per requirements we create dashboard,
         charts, graphs and query data using PromQL 

10. explain the Architecture how you implement the Monitoring 
    1. Define Objectives and SLI:
    2. Selecting Monitoring Tools: 
    3. Instrument the application:
    4. Set-up Prometheus:
    5. Implement Alerting:
    6. Visualize with Grafana: 
    7. Continuous Monitoring and Scaling:
    8. Incident Response:
    9. Capacity Planning:

11. docker file for java application:
    FROM openjdk:11-jre-slim 
    WORKDIR /app
    COPY target/your-app.jar . 
    ENTRYPOINT ['java', '-jar', 'your-app.jar'] 

12. difference between SRE and Devops
    SRE: SRE is a set of practice and principle developed by google to ensure reliability and availability
         of large-sclaed systems and servises. They focuse on SLO and SLI that defines desired reliability
         of the servises and set targets with respect to it and aims to meet the targets.
         SRE work under data-driven approach and closely work with software developers.
    DEvops: Devops are like mainly aims to bridge gap between development and operation teams and mainly focus 
            Automation and speed up the software delivery process. Devops dont gives more importances to
            site reliability and all. they just aim to speed the softare deployment process.

13. How to connect from browser to application running inside a private subnet.
    => we can achieve it by creating ELB in public subnet of that VPC and in 
       Target Groups selecting those private instance.

14. How to bring changes made manually to aws in terraform statefile 
    => we can use terraform import command to add the new entries in terraform statefile 
   
15. what will happen if provisioned infrastructure using terraform is manually made some changes by someone
    and you do terraform apply for new changes.
    => Terraform will detect the changes made in the console and will attempt to reconcile them with the 
       changes defined in your Terraform configuration. This means that Terraform may destroy 
       and recreate resources, or it may update them in-place.
    => If there are any conflicts between the changes made in the console and the changes defined in your
       Terraform configuration, Terraform will stop and ask you to resolve the conflicts manually. This 
       is to prevent Terraform from making unexpected changes to your infrastructure.
    => Once you have resolved any conflicts, you can re-run terraform apply and Terraform will apply 
       the changes defined in your configuration.

    To handle this situation:
    => Import Resources: You can import manually created resources into Terraform's state. This allows 
       Terraform to manage those resources going forward. However, care must be taken to ensure that the 
       state in Terraform accurately reflects the actual state of resources.
    => Plan and Review: Always perform a terraform plan before applying changes. This will show what 
       Terraform intends to do. Review this plan carefully to understand how Terraform will reconcile 
       the differences between the desired state in your configuration files and the actual state of 
       resources.
    => Collaboration and Documentation: Ensure team members are aware of Terraform's management and that
       manual changes should be communicated and ideally integrated back into Terraform's configuration.

16. if terraform statefile get deleted and you do terraform apply again, what will happen?
    => Terraform will think that all the infrastructure has been destroyed and think it as a new terraform
       script and will try to recreate it from scratch and result in duplication or conflicts.

       There are a few things you can try to recover it:
       => If you have a backup of the state file, you can restore it.
       => If you do not have a backup of the state file, you can try to recreate it from scratch using the 
          terraform import command.
       => If you cannot recreate the state file, you will need to destroy all of your infrastructure and 
          then re-provision it using Terraform.

17. if customer not able to access application through url, how to resolve it.
    Involve several steps:
    => Check Application Status:
       Verify if the application is running and accessible internally.
       Check application logs for any errors or warnings that might indicate issues.

    => DNS Resolution:
       Confirm if the URL is resolving to the correct IP address.
       Check DNS settings for any misconfigurations or DNS resolution issues.

    => Load Balancer or Proxy:
       If there's a load balancer or proxy in front of the application, check its configuration and logs 
       for any issues.
       Verify if the load balancer is distributing traffic as expected.

    => Security and Firewall:
       Review security groups, firewalls, and access control lists to ensure they aren't blocking 
       incoming traffic.
       Check for any recent security updates or changes that might have affected access.
       
    => Application Code and Configuration:
       Examine the application code for errors or recent changes that might have introduced issues.
       Validate the application's configuration files for correctness.

    => Resource Utilization:
       Monitor resource utilization (CPU, memory, disk space) on servers or containers to ensure they're 
       not overloaded.

    => Service Dependencies:
       Check if any external services or dependencies (databases, APIs) that the application relies on 
       are accessible and functioning correctly.

    => Incident Response and Communication:
       If the issue identified, initiate an incident response process, involving relevant team members or 
       escalation paths.
       Communicate with stakeholders, keeping them informed about the investigation and steps being 
       taken to resolve the issue.
      
    => Test the fix: 
       Once you have fixed the problem, you need to test it to make sure that it is fixed. 
       This may involve trying to reproduce the problem again or asking the customer to try to access 
       the application again.

    => Logging and Monitoring:
       Ensure robust logging and monitoring systems are in place to gather data for troubleshooting and 
       post-incident analysis.

    => Documentation and Post-Incident Analysis:
       Document all steps taken during the investigation and resolution process.
       Conduct a post-incident analysis to identify the root cause and implement preventive measures to 
       avoid similar issues in the future.

18. how to create a self managed kubernetes cluster
    => we have used kubeadm tool to create kubernetes cluster manually  
    => before all the we have to provisions infrastructure as per requirements by creating vpc with cidr range 
       and launging instances in multi AZ's 
    => we had return a scripts to provision both master nodes and worker nodes on instances, we just scripts
       on them 
    => Install Kubernetes Components:
       Install kubeadm, kubectl, and kubelet on all nodes.
       Initialize the control plane node or master node using kubeadm init.
       Join worker nodes to the cluster using the token generated by kubeadm init.
    => we are using quoram calculation to calculate number of worker nodes to ensure high avaialability for
       kubernetes cluster
    => we usually maintain 3 master nodes for lower environment and 5 to 7 master nodes higher environments
    => we had around 20-25 worker nodes in our organization 
    
19. if kubernetes scheduler get crashed, how to resolve it
    => 1.Check Scheduler Status:
          Confirm if the scheduler is actually not running by checking its status using:
            kubectl get pods -n kube-system
          Look for the scheduler pod and its status (Running, CrashLoopBackOff, Error, etc.).
       2.Restart the Scheduler Pod:
          If the scheduler pod is not running or has errors, try restarting it:
          kubectl delete pod -n kube-system <scheduler-pod-name>
         Kubernetes will automatically recreate the scheduler pod.
       3.Examine Scheduler Logs:
          Check the logs of the scheduler pod to identify any errors or issues that might be causing the
          crash:
          kubectl logs -n kube-system <scheduler-pod-name>
          Analyze the logs for any specific error messages or warnings.
       4.Check Node Health and Resources:
          Ensure the nodes in the cluster are healthy and have enough resources (CPU, memory, disk) 
          available for scheduling new pods.
          Check if there are any node-specific issues impacting scheduling.
       5.Restart Kubernetes Components:
          If restarting the scheduler doesn't resolve the issue, consider restarting other relevant 
          Kubernetes control plane components such as kube-controller-manager or kube-apiserver.
       6.Investigate Configuration:
          Review the configuration of the scheduler for any misconfigurations that might be causing 
          problems. Compare it with a known working configuration.
       7.Update Kubernetes Version:
          Consider upgrading Kubernetes to the latest stable version. Sometimes, issues with components 
          are resolved in newer versions.

20. how to recover data from pods running in worker nodes, if kubernetes cluster gets down
    => 1.Identify the type of persistent storage:
         => Persistent storage: Data stored in persistent storage is preserved even when the pod 
                                terminates. Persistent storage options include:
               Local storage: Data is stored on the local filesystem of the worker node.
               Cloud storage: Data is stored in a cloud-based storage service.
   => 2.Locate the persistent storage:
        => Local storage: The data is located on the local filesystem of the worker node. You can access 
           the data by physically accessing the worker node or mounting the filesystem remotely.
           Cloud storage: The data is located in the cloud storage service. You can access the data using 
           the cloud storage service's API or client applications.

   => 3.Recover the data:
        => Local storage: Copy the data from the worker node's filesystem to another location.
           Cloud storage: Download the data from the cloud storage service to another location.

   Here are some additional considerations for recovering data from pods running in worker nodes:
      Backups: Regularly back up your data to prevent data loss in case of cluster failures or other 
              incidents.
      Replication: Use replication mechanisms to ensure that data is stored on multiple nodes to improve 
              availability and resilience.
      Disaster recovery: Develop a disaster recovery plan that includes procedures for recovering data 
             and applications in case of major outages.
 